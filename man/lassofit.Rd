% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lasso.R
\name{lassofit}
\alias{lassofit}
\title{Fit Gaussian linear model with lasso regularization}
\usage{
lassofit(
  x,
  y,
  nfolds = 10,
  foldid = NULL,
  lambda = NULL,
  nlambda = 100,
  type.measure = c("mse", "mae"),
  parallel = FALSE,
  criterion = c("cv", "aic", "bic"),
  standardize = FALSE,
  standardize.response = FALSE,
  sigma2 = NULL,
  lambda.min.ratio = 1e-04,
  betatypes = NULL
)
}
\arguments{
\item{x}{A standardized matrix of predictors of dimension nobs x nvars, where
each row is an observation vector.If not standardized, set standardize to TRUE.}

\item{y}{A numeric centered quantitative vector of response. If not centered,
set `standardize` to TRUE; this will standardize x but center y. See `standardize.response`
for more details.}

\item{nfolds}{Number of folds for cross-validation. Default is 10. Smallest value
allowable is nfolds = 3.}

\item{foldid}{an optional vector of values between 1 and nfolds identifying
what fold each observation is in. If supplied, nfolds can be missing.}

\item{lambda}{Optional user-supplied lambda sequence. Default is NULL, and
the program chooses its own sequence assuming x and y are standardized.}

\item{nlambda}{The number of lambda values.Default is 100}

\item{type.measure}{Loss function to use for cross-validation. Currently,
two options are available. The default option is \code{type.measure = "mse"},
which corresponds to squared error. Another option is \code{type.measure = "mae"}
(mean absolute error).}

\item{parallel}{The program use \code{"[cv.glmnet()]"} function which supports
parallel computing. To make it work, users must register parallel beforehand.
see `glmnet` package for details}

\item{criterion}{The criterion used to select tuning parameters for the
lasso regression. Available options include: \code{criterion = "cv"}
which perform cross-validation to select the optimal tuning parameter, while
\code{criterion = "aic"} and \code{criterion = "bic"} select the tuning parameter
 based on the AIC and BIC, respectively.}

\item{standardize}{Specifies whether to standardize the predictors matrix
\code{`x`} to have mean 0 and unit variance. If set to TRUE, \code{`x`} will be
standardized. Additionally, the response variable \code{`y`} will be centered
by subtracting its mean. This standardization step can be useful to ensure
that the predictors are on a comparable scale. By default,
\code{standardize = FALSE}, indicating that no standardization will be performed.
This assumes that users have already standardized x and centered y so that the intercept
is zero.}

\item{standardize.response}{Specifies whether the response variable y should be
standardized to have unit variance. This option divides \code{`y`} by its standard deviation.}

\item{sigma2}{The residual variance obtained by fitting the full OLS model without
any regularization or feature selection. It is used for the calculation of AIC and BIC.
To compute these metrics, the residual variance needs to be provided via the
`sigma2` parameter.}

\item{lambda.min.ratio}{When lambda values are automatically generated, the sequence
is determined by lambda.max and lambda.min ratio. The program generates nlambda values
on the log scale from lambda.max down to lambda.min. lambda.max is not user-specified but is
computed from the input standardized x and y. It is the smallest value for lambda such that
all the coefficients are zero. The default is lambda.min.ratio = 0.0001.}

\item{betatypes}{Not used but added for consistency with the oracle_model().}
}
\value{
Returns the following items:
\item{beta:}{Shrunken regression estimates without an intercept.}
\item{lambda:}{The tuning parameter used for the estimation of parameters.}
\item{gamma:}{Not applicable for lasso but returned for consistency with other methods like adaptive lasso.}
\item{x:}{A standardized matrix of predictors used in fitting the linear model.}
\item{y:}{A centered vector of the response variable used in fitting the linear model.}
}
\description{
Fits the Lasso model using the optimal tuning parameter
determined through cross-validation. It also supports the selection
of tuning parameters based on the Akaike Information Criterion (AIC) or
Bayesian Information Criterion (BIC). The value of lambda value that yield the
smallest AIC or BIC is chosen as the best tuning parameter.
}
